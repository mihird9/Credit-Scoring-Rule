---
title: "Classification_German Credit"
author: "Mihir Dixit"
date: "1/30/2020"
output: rmarkdown::github_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse=TRUE, comment="##", fig.retina=2, fig.path = "README_figs/README-")
```

Loading necessary packages.

```{r message=FALSE}
library(rpart)
library(rpart.plot)
library(readxl)
library(dplyr)
library(tidyverse)
library(pastecs)
library(psych)
library(ggplot2)
library(knitr)
library(stringr)
library(DT)
library(corrplot)
library(caret)
library(MLmetrics)
library(ROCR)
```

Loading data into RStudio. 
Take note that it is a good practice to avoid hardcoding the data.

```{r}
input_file = "GermanCredit_assgt1_F18.xls"
mdData <- read_excel(input_file, col_names = TRUE)
View(mdData)
```

Let's convert the data into a tibble and then check do a quick check for the data summary, structure and class.

```{r}
mdData <- as_tibble(mdData)
summary(mdData)
str(mdData)
class(mdData)
```
```{r include=FALSE}
options(scipen=999)
options(digits=4)
```

Let's look at the summary statistics of the data.

```{r}
describe(mdData)
```

Let's see all the column names to see if we need to make any changes.

```{r}
colnames(mdData)
```

Renaming the column "RADIO/TV".
```{r}
mdData <- rename(mdData, Radio.TV =`RADIO/TV`)
```

Checking columns for presence of missing values.
```{r}
colSums(is.na(mdData))
```

There are 7 columns with NAs.
Replacing NAs by 0s in all columns except AGE. 
```{r}
mdData$NEW_CAR[is.na(mdData$NEW_CAR)] <- 0
mdData$USED_CAR[is.na(mdData$USED_CAR)] <- 0
mdData$FURNITURE[is.na(mdData$FURNITURE)] <- 0
mdData$Radio.TV[is.na(mdData$Radio.TV)] <- 0
mdData$EDUCATION[is.na(mdData$EDUCATION)] <- 0
mdData$RETRAINING[is.na(mdData$RETRAINING)] <- 0

colSums(is.na(mdData))
```

```{r}
summary(mdData$AGE)
head(mdData$AGE)
dim(mdData)
```

Let's have a look at the distribution of AGE.
```{r}
hist(mdData$AGE, xlab = "AGE", main = "Distribution of Age")

ggplot(data=mdData, aes(mdData$AGE)) + geom_histogram(breaks=seq(15, 80, by=5), col= "black", fill="steelblue")
```

Replacing missing values in AGE with the mean age.
```{r}
mdData$AGE[is.na(mdData$AGE)] <- round(median(mdData$AGE, na.rm = TRUE))

colSums(is.na(mdData))
summary(mdData$AGE)
```

Assigning a new object to the final dataset.
```{r}
dataset <- mdData
str(dataset)
colnames(dataset)
```

We will consider the below variables as factor variables (and not numeric) as they have 
integer values and the max values are pretty low.

```{r}
max(mdData$NUM_DEPENDENTS)
max(mdData$NUM_CREDITS)
```

Converting to factor Variables:
```{r}
#removing these numeric columns to extract all other factor columns.
col<-dataset[-c(1,3,11,23)]

colnames<-names(col)
dataset[colnames]<-lapply(dataset[colnames],factor)
sapply(dataset,class)

```

### Univariate Analysis

#### Univariate Analysis of Response
```{r}
# Bar chart in R base.
tab <- table(dataset$RESPONSE)
tab
barplot(prop.table(tab), xlab = "Response", ylab = "Proportion")
```

Creating a proportion table (class dataframe) for use with ggplot.
```{r}

tmp<-dataset%>% 
  group_by(RESPONSE) %>%
  summarise(count = n()) %>%
  mutate(Proportion = count/sum(count))

tmp

# Barchart in ggplot
tmp %>% 
    ggplot(aes(x=RESPONSE,y = Proportion, fill=RESPONSE))+
  geom_bar(stat="identity")
```

### Univariate Analysis

Distribution of Age
```{r}
ggplot(data=mdData, aes(mdData$AGE)) + geom_histogram(breaks=seq(15, 80, by=5), col= "black", fill="steelblue")
```

Distribution of Duration 
```{r}
ggplot(data=mdData, aes(mdData$DURATION)) + geom_histogram(breaks=seq(0, 80, by=10), col= "black", fill="steelblue")
```

Distribution of Amount
```{r}
ggplot(data=mdData, aes(mdData$AMOUNT)) + geom_histogram(breaks=seq(250, 20000, by=500), col= "black", fill="steelblue")
```

### Bivariate Analysis: Categorical variables

#### Response against History
```{r}

table(dataset$HISTORY, dataset$RESPONSE)
prop <- prop.table(table(dataset$HISTORY, dataset$RESPONSE), 1)

colnames(prop) <- c("Bad", "Good")
rownames(prop) <- c("No Credits", "All paid back", "Existing paid back", "Delay in payback", "Critical acc")
prop

dat <- data.frame(prop)
names(dat) <- c("HISTORY", "RESPONSE", "Proportion")
ggplot(data=dat, aes(x=HISTORY, y=Proportion, fill=RESPONSE)) + geom_bar(stat="identity")
```

#### Response against Employment
```{r}


table(dataset$EMPLOYMENT)
prop <- prop.table(table(dataset$EMPLOYMENT, dataset$RESPONSE), 1)

colnames(prop) <- c("Bad", "Good")
rownames(prop) <- c("Unemployed", "Less than 1 yr", "1-4 yrs", "4-7 yrs", "More than 7 yrs")
prop

dat <- data.frame(prop)
names(dat) <- c("EMPLOYMENT", "RESPONSE", "Proportion")
ggplot(data=dat, aes(x=EMPLOYMENT, y=Proportion, fill=RESPONSE)) + geom_bar(stat="identity")
```

#### Response against Guarantor
```{r}
table(dataset$GUARANTOR)
prop <- prop.table(table(dataset$GUARANTOR, dataset$RESPONSE), 1)
colnames(prop) <- c("Bad", "Good")
rownames(prop) <- c("No", "Yes")
prop

dat <- data.frame(prop)
names(dat) <- c("GUARANTOR", "RESPONSE", "Proportion")
ggplot(data=dat, aes(x=GUARANTOR, y=Proportion, fill=RESPONSE)) + geom_bar(stat="identity")
```

#### Response against Real Estate
```{r}
table(dataset$REAL_ESTATE)
prop <- prop.table(table(dataset$REAL_ESTATE, dataset$RESPONSE), 1)

colnames(prop) <- c("Bad", "Good")
rownames(prop) <- c("No", "Yes")
prop

dat <- data.frame(prop)
names(dat) <- c("REAL_ESTATE", "RESPONSE", "Proportion")
ggplot(data=dat, aes(x=REAL_ESTATE, y=Proportion, fill=RESPONSE)) + geom_bar(stat="identity")
```


```{r}
table(dataset$CHK_ACCT)
prop <- prop.table(table(dataset$CHK_ACCT, dataset$RESPONSE), 1)
colnames(prop) <- c("Bad", "Good")
rownames(prop) <- c("Less than 0 DM", "1 DM to 200 DM", "Above 200 DM", "No Checking Account")
prop

dat <- data.frame(prop)
names(dat) <- c("Checking_Account_status", "RESPONSE", "Proportion")
ggplot(data=dat, aes(x=Checking_Account_status, y=Proportion, fill=RESPONSE)) + geom_bar(stat="identity")
```

### Bivariate Analysis: Numeric variables

#### Response against Age
```{r}
ggplot(data=dataset, aes(x=RESPONSE, y=AGE, fill=RESPONSE)) + geom_boxplot()
```

#### Response against Duration
```{r}
ggplot(data=dataset, aes(x=RESPONSE, y=DURATION, fill=RESPONSE)) + geom_boxplot()
```

#### Response against Amount
```{r}
ggplot(data=dataset, aes(x=RESPONSE, y=AMOUNT, fill=RESPONSE)) + geom_boxplot()
```



Now let's check for any correlation between the numeric variables.
```{r}
correlation_Spearman<-cor(subset(dataset, select =  c(3,11,23)), use="complete.obs", method="spearman")
corrplot(correlation_Spearman,method= "circle",tl.cex = 0.8,tl.offset = 1.5,number.cex = 0.7,type = 'upper')
```

### Feature Selection
```{r}
dataset_2<- dataset[,sapply(dataset,nlevels)>1] # selecting only the factor variables
dataset_numeric<-subset(dataset, select =  c(3,11,23)) # selecting the numeric variables
```

Now let us see the RESPONSE variable is dependent on which of the factor variables on i.e. which factor variables have an association with the RESPONSE variable.
Since both the variables being compared are categorical, we use the chi-squared test for the same.

We then generate list of all factor variables that are found to have associations with the response variable.
```{r}
List_Significant_flags<-list()

for (i in names(dataset_2))
{
  
  if(!is.numeric(dataset_2[i]))
    
  {
    chisq<-chisq.test(dataset_2[i],dataset_2$RESPONSE)
    
    if(chisq$p.value <0.05)
    {
      print(i)
      List_Significant_flags<-c(List_Significant_flags,i)
      out <- capture.output(print(chisq))
      i<-capture.output(print(i))
      #
     cat(out, sep="\n", append=TRUE)
      
    }
  }# print(chisq$residuals)
}
```

Creating the final dataset for modelling. 
```{r}
myvars<- names(dataset) %in% List_Significant_flags | names(dataset) %in% names(dataset_numeric)

dataset_final <- dataset[,myvars]
dim(dataset_final)

```

### Decision Tree Model using the CART algorithm (rpart package)
Let's start with the most common decision tree algorithm i.e. CART 

#### Model 1: Using all attributes in the original dataset
The default splitting criteria for classification is "gini".
```{r}
rpModel1 <- rpart(RESPONSE ~ ., data=dataset, method="class")
print(rpModel1)

summary(rpModel1)

prp(rpModel1, type = 3, under = TRUE, extra = 106, fallen.leaves = FALSE, tweak = 2.5, box.palette = "auto", main = "Decision Tree Model using all attributes", under.cex = 1, clip.right.labs = FALSE, branch = 0.3)
```

Making prediction on the dataset
```{r}
predfull <- predict(rpModel1, dataset, type='class')

table(predicted = predfull, actual=dataset$RESPONSE)
```

Accuracy of the model on the whole dataset
```{r}
mean(predfull==dataset$RESPONSE) # Accuracy
```

#### Model 1.1 
Using the final dataset but without train/test split
```{r}
rpModel1.1 <- rpart(RESPONSE ~ ., data=dataset_final, method="class")
summary(rpModel1.1)

pred_df <- predict(rpModel1.1, dataset_final, type='class')

table(predicted = pred_df, actual=dataset_final$RESPONSE)

mean(pred_df==dataset_final$RESPONSE) # Accuracy
```

#### Model 2: Model using final dataset (splitting criteria: gini)

Splitting the dataset into training(70%) and test(30%) datasets
```{r}
nr <- nrow(dataset)

trnIndex <- sample(1:nr, size = round(0.7*nr), replace=FALSE) 

mdTrn <- dataset_final[trnIndex,] 
mdTst <- dataset_final[-trnIndex,]
```

Creating Decision Tree Model 2
Let's create a simple decision tree using the method "class". When no parameters are mentioned, the splitting criteria defaults to "gini".
```{r}
rpModel2 <- rpart(RESPONSE ~ ., data=mdTrn, method="class")

prp(rpModel2, type = 3, under = TRUE, extra = 106, fallen.leaves = FALSE, tweak = 2.5, box.palette = "auto", main = "Decision Tree Model using final dataset", under.cex = 1, clip.right.labs = FALSE, branch = 0.3)
```

Making prediction on the training set
```{r}
predTrn <- predict(rpModel2, mdTrn, type='class')

table(predicted = predTrn, actual=mdTrn$RESPONSE)
```

Accuracy of Model 2 on training dataset
```{r}
mean(predTrn==mdTrn$RESPONSE) # Accuracy
```

Making prediction on the test set
```{r}
predTst <- predict(rpModel2, mdTst, type='class')

table(predicted = predTst, actual=mdTst$RESPONSE)
```

Accuracy of Model 2 on test dataset
```{r}
mean(predTst==mdTst$RESPONSE) # Accuracy
```

#### Model 3: Model using final dataset (splitting criteria: information gain)

```{r}
rpModel3 <- rpart(RESPONSE ~ ., data=mdTrn, method="class", parms = list(split = "information"))

prp(rpModel3, type = 3, under = TRUE, extra = 106, fallen.leaves = FALSE, tweak = 2.5, box.palette = "auto", main = "Decision Tree Model using final dataset", under.cex = 1, clip.right.labs = FALSE, branch = 0.3)
```

Making prediction on the training set
```{r}
predTrn <- predict(rpModel3, mdTrn, type='class')

table(predicted = predTrn, actual=mdTrn$RESPONSE)
```

Accuracy of Model 3 on training dataset
```{r}
mean(predTrn==mdTrn$RESPONSE) # Accuracy
```

Making prediction on the test set
```{r}
predTst <- predict(rpModel3, mdTst, type='class')

table(predicted = predTst, actual=mdTst$RESPONSE)
```

Accuracy of Model 3 on test dataset
```{r}
mean(predTst==mdTst$RESPONSE) # Accuracy
```

#### Model 4: Model using final dataset (splitting criteria: gini, )

```{r}
rpModel4 <- rpart(RESPONSE ~ ., data=mdTrn, method="class", control = rpart.control(minsplit = 10, maxdepth = 8, maxsurrogate = 0, xval = 2, minbucket = 10))

prp(rpModel4, type = 3, under = TRUE, extra = 106, fallen.leaves = FALSE, tweak = 2.5, box.palette = "auto", main = "Decision Tree Model using final dataset", under.cex = 1, clip.right.labs = FALSE, branch = 0.3)
```


```{r}
predTrn <- predict(rpModel4, mdTrn, type='class')

table(predicted = predTrn, actual=mdTrn$RESPONSE)
```


```{r}
mean(predTrn==mdTrn$RESPONSE) # Accuracy
```


```{r}
predTst <- predict(rpModel4, mdTst, type='class')

table(predicted = predTst, actual=mdTst$RESPONSE)
```


```{r}
mean(predTst==mdTst$RESPONSE) # Accuracy
```


```{r}
predTrnProb <- predict(rpModel2, mdTrn, type='prob')
head(predTrnProb)
```



```{r}
trnSc <- subset(mdTrn, select=c("RESPONSE"))  # selects the OUTCOME column into trnSc
trnSc["score"]<-predTrnProb[, 1]  

trnSc<-trnSc[order(trnSc$score, decreasing=TRUE),]
trnSc$RESPONSE <- as.numeric(trnSc$RESPONSE)
trnSc$cumDefault <- cumsum(trnSc$RESPONSE)
head(trnSc)
plot(seq(nrow(trnSc)), trnSc$cumDefault,type = "l", xlab='#cases', ylab='#default')
```

```{r}
cm <- table(predicted = predTst, actual = mdTst$RESPONSE) # Confusion Matrix
cm
n <- sum(cm) # number of instances
d <- sum(diag(cm)) # number of correctly classified instances per class
```

```{r}
cm[2,2]
```



```{r}
Accuracy(y_pred = predTst, y_true = mdTst$RESPONSE)

Precision(y_pred = predTst, y_true = mdTst$RESPONSE, positive = "1")

Recall(y_pred = predTst, y_true = mdTst$RESPONSE, positive = "1")

F1_Score(y_pred = predTst, y_true = mdTst$RESPONSE, positive = "1")

CTHRESH=0.5

predProbTrn <- predict(rpModel2, mdTrn, type='prob')
#Confusion table
predTrn <- ifelse(predProbTrn[,'1'] >= CTHRESH, '1', '0')
ct <- table( predicted = predTrn, actual = mdTrn$RESPONSE)
ct

#Accuracy
mean(predTrn==mdTrn$RESPONSE)
Accuracy(y_pred = predTrn, y_true = mdTrn$RESPONSE)
```


```{r}
library(ROCR)
#score test data set
mdTst$score <-predict(rpModel2,type='prob',mdTst)
pred <- prediction(mdTst$score[,2],mdTst$RESPONSE)
perf <- performance(pred,"tpr","fpr")
plot(perf)
```

```{r}
costMatrix <- matrix(c(0,1,5, 0), byrow=TRUE, nrow=2)
colnames(costMatrix) <- c('Predict Good','Predict Bad')
rownames(costMatrix) <- c('Actual Good','Actual Bad')
costMatrix
```

```{r}
rpTree <- rpart(RESPONSE ~ ., data=mdTrn, method="class", parms = list( prior = c(.70,.30), loss = costMatrix, split = "information"))

th <- costMatrix[2,1]/(costMatrix[2,1] + costMatrix[1,2])
th
```

